{
 "metadata": {
  "name": "",
  "signature": "sha256:6afb1a6ff5bb8d66f18ce18bc5d2864924b5906b265e84748c2e96bd10a60baf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**AM 207**: Homework 6"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_ _ _ _ _\n",
      "\n",
      "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
      "Handed out: Thursday, March 12th, 2015<br>\n",
      "Due: 11.59 P.M. Friday March 28th, 2015\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "+ Upload your answers in an ipython notebook to the dropbox on iSites, you can find the link on our homepage (http://am207.org/).\n",
      "\n",
      "+ We will provide you imports for your ipython notebook. For the first part of this course, please do not import additional libraries.\n",
      "\n",
      "+ Your individual submissions should use the following filenames: AM207_LASTNAME_FIRSTNAME_HW6.ipynb\n",
      "\n",
      "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
      "\n",
      "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
      "\n",
      "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_LASTNAME_FIRSTNAME_HW0.tar.gz or AM207_LASTNAME_FIRSTNAME_HW0.zip\n",
      "\n",
      "+ Please remember that your solution should be a **report**! We would like some explanations of your ideas, and ways to approach the solution. Also please comment your code. \n",
      "\n",
      "+ THERE WILL BE NO LATE DAYS FOR THIS HW! \n",
      "\n",
      "### Have Fun!\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do NOT Add to this list without prior approval\n",
      "\n",
      "import time\n",
      "import math\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy\n",
      "from statsmodels.graphics.gofplots import qqplot_2samples\n",
      "\n",
      "\n",
      "# for pure convenience\n",
      "from scipy.stats import norm\n",
      "\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# you don't have to use seaborn if you prefer plain matplotlib\n",
      "import seaborn as sns\n",
      "sns.set_style('white')\n",
      "sns.set_context('paper')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 1:  The Best Thing Since Sliced Sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's understand slice sampling by seeing some of the quirks.  \n",
      "(a) Below you'll find a bad slice sampler named bad_slice_sampler.  Show that it works perfectly for unimodal distributions (guassians, cauchy, etc..).  How do you compare to the true distribution?  * **Hint:  You can get your distribution from scipy.stats and then use scipy.stats.probplot.  You can also plot the normed histogram.** * \n",
      "\n",
      "(b) Show that it doesn't work as well on multimodal distributions.  One suggestion is the gaussian mixture from lecture which we've defined below as the function ex2.  Draw 5000 samples and with ex2 I suggest a width of 0.2, but experiment with widths.  How does width affect sampling performance? \n",
      "\n",
      "(c) Fix bad_slice_sampler to get a correct univariate slice sampling implementation.  Call it good_slice_sampler and use the function skeleton we give you below. What needs to be changed and why does that affect sampling performance?  Draw true samples for the mixture distribution (how might you sample from a mixture distribution) and compare to both you sampling implementations.  * **Hint: if you have statsmodels installed, you can use qqplot_2samples to create a qqplot based on samples.  What should that look like?  What does it look like for bad_slice sampler? ** *\n",
      "\n",
      "(d) * **Challenge -- Optional, but without this you won't be able to get a 5 on this HW** *: Implement a 2 dimensional slice sampler using the hyper-rectangle method.  Use the function skeleton given below. Implement (using the function skeleton given below) a 2 dimensional metropolis-hastings sampler.  How do the two compare with respect to speed, effective sample sizes, autocorrelation, and traceplots on the following form of a Banana PDF from HW4 (use a sample size of 10000 samples):\n",
      "\n",
      "$ p(X) \\propto {\\rm exp} \\left[ - \\frac{1}{2a^2} \\left(\\sqrt{x_1^2 + x_2^2} -1 \\right)^2 -  \\frac{1}{2b^2} \\left(x_2  - 1 \\right)^2  \\right] $ where $a=0.1$ and $b=1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu1=3; mu2=10; sigma1=1; sigma2=2; l1=.30; l2=.70;\n",
      "ex = lambda x: 0.5*norm.pdf(x,loc=-1,scale=0.3)+ 0.5*norm.pdf(x,loc=1,scale=0.3)\n",
      "ex2 = lambda x: l1*norm.pdf(x, mu1, sigma1)+l2*norm.pdf(x, mu2, sigma2)\n",
      "\n",
      "def bad_slice_sampler(pdf, x0, width, division=0.5, sample_size=5000, burnin=0, thin=0):\n",
      "\n",
      "    # initialize the sampler\n",
      "    y0 = np.random.uniform(low=0,high=pdf(x0))\n",
      "    samples = []\n",
      "\n",
      "    # create an interval around the original point\n",
      "    #init_unif = np.random.uniform(low=0,high=width)\n",
      "    left = x0 - division*width\n",
      "    right = left + width\n",
      "\n",
      "    # slice sampling magic\n",
      "    for i in range(sample_size + burnin):\n",
      "\n",
      "        # select our interval\n",
      "        while(y0 < pdf(left)): # left endpoint\n",
      "            left = left - width\n",
      "\n",
      "        while (y0 < pdf(right)): # right endpoint\n",
      "            right = right + width\n",
      "\n",
      "        while (True):\n",
      "            # new proposal\n",
      "            xstar  = np.random.uniform(low=left, high=right)\n",
      "\n",
      "            # loop until we find the point inside the region\n",
      "            if y0 < pdf(xstar):\n",
      "                break\n",
      "            else:\n",
      "                # shrink interval\n",
      "                if xstar < x0:\n",
      "                    left = xstar\n",
      "                if xstar > x0:\n",
      "                    right = xstar\n",
      "\n",
      "        # save the current sample\n",
      "        samples.append(xstar)\n",
      "\n",
      "        # the last sample is now the proposal\n",
      "        x0 = xstar\n",
      "\n",
      "        # get the new y0 for next step\n",
      "        y0 = np.random.uniform(low=0, high=pdf(x0))\n",
      "\n",
      "        # reset the interval around x0\n",
      "        #init_unif = np.random.uniform(low=0,high=width)\n",
      "        left = x0 - division*width\n",
      "        right = left + width\n",
      "\n",
      "\n",
      "    return np.array(samples[burnin:])\n",
      "\n",
      "def good_slice_sampler(pdf, x0, width, sample_size=5000, burnin=0, thin=0):\n",
      "    \n",
      "    ## Fix bad_slice_sampler here \n",
      "    pass\n",
      "\n",
      "def metropolis_hastings_sampler(pdf,x0,sample_size=5000,burnin=0, thin=0):\n",
      "    \n",
      "    ## Metropolis Hastings here\n",
      "    pass\n",
      "\n",
      "def slice_sampler_2d(pdf, x0, width, sample_size=5000, burnin=0, thin=0):\n",
      "    \n",
      "    ## Implement 2d slice sampler here\n",
      "    ## see lecture 11 notes for a head start\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 2:  Hierarchical Modeling is No Victorian Secret"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have (from Bayesian Data Analysis, Gelman et. al.) a dataset of kidney cancer death rates by county in the US.  Let's assume that we can use a Poisson distribution to model the per county kidney death rate\n",
      "\n",
      "$$ y_j \\sim Poisson(10n_j\\theta_j) \\\\\n",
      " \\theta_j \\sim Gamma(\\alpha, \\beta)$$.\n",
      "\n",
      "where $n_j$ is the total population of county j, $y_j$ is the number of kidney cancer deaths in county j and $\\theta_j$ is the underlying death rate for county j.  A Poisson model makes sense because kidney cancer incidence should be a rare event (we hope!) and the Poisson is a good distribution for modeling rare events.  The gamma is a distribution with the right support that is conveniently the conjugate distribution for the Poisson. Before we were exposed to hierarchical modeling, we'd be inclined to use \"full pooling\" and choose one prior distribution for $\\theta$ for all the counties.  Now that we have access to hierarchical modeling let's be a bit more sophisticated.  Let's define a hierarchical model where the $\\alpha$ and $\\beta$ are also drawn from gamma distributions as well.  Our full hierarchical model is defined below:\n",
      "$$ y_j \\sim Poisson(\\lambda_j) \\\\\n",
      " \\theta_j \\sim Gamma(\\alpha, \\beta)\\\\\n",
      " \\alpha \\sim Gamma(a_{\\alpha},b_{\\alpha})\\\\\n",
      " \\beta \\sim Gamma(a_{\\beta},b_{\\beta})$$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(a) Show that a Gamma is conjugate prior to a Poisson and write out the joint posterior density.\n",
      "\n",
      "(b) Similar to what we did in lecture with the rat tumor and the beta binomial model, we can find full conditional distributions for $\\lambda_j$, $\\alpha$ and $\\beta$.  Derive them.\n",
      "\n",
      "(c) How might you choose your prior hyperparameters $\\{a_{\\alpha},\\ b_{\\alpha},\\ a_{\\beta},\\ b_{\\beta}\\ \\}$?\n",
      "\n",
      "(d) You should have two conditional distributions that you need to sample from with MCMC (with some clever manipulations you may have just one) and one conditional distribution that you can sample from a Gamma.  Run a Gibbs sampler with the county data in <a href=\"HW6.csv\">this file</a>.  (Feel free to use a subset of counties, e.g. the counties an individual state like Texas.  Only run the whole data set if you're in an ambitious \"I'm at the top of the hiearchy\" mood).  You'll want the columns death_count, population, and maybe (if you don't want to calculate it yourself) mortality_rate.  Use your good_slice_sampler (or as an alternative recycle a MH sampler from a previous HW) for drawing samples from the distributions that aren't from a known model.  Give posterior plots, posterior metrics and traceplots/other convergence criteria.  * ** Beware:  The gamma distribution in scipy.stat uses a different convention than we're used to.  It uses b as a scale parameter and you enter that parameter as the reciprocal of the normal scale parameter.  So, the pdf of $Gamma(a,b)$ in scipy would be scipy.stats.gamma.pdf(x, a, scale=1./b) ** *\n",
      "\n",
      "One of the advantages of the Poisson Gamma hiearchical model in this case is that it acts almost like a regularizer for population disparities.  Counties with small populations will be highly influenced by the prior, whereas counties with high population (high data counts) will be influenced by the data. What kinds of suggestion do you have for improving the prior?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}